# ======================================================
# Train + Test Speech Enhancement (for PyCharm)
# Model: UNetBiGRU (from net.py)
# ======================================================

import os, glob, random, torch, torch.nn as nn, torch.nn.functional as F
import numpy as np
import soundfile as sf
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from GDPTCN import UNetDPT
from Net import UNetBiGRU
from torch.amp import GradScaler, autocast
from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality
from torchmetrics.audio.stoi import ShortTimeObjectiveIntelligibility

# -------------------- fixed config --------------------
# ==== user-editable ====
DATA_ROOT = r"E:\VOICEBANK_ROOT\VOICEBANK_ROOT"
OUT_DIR = r"E:\speech\out"
CKPT_PATH = r"E:\checkpoint\ckpt_final.pth"
EPOCHS = 50
BATCH_SIZE = 32
LR = 1e-3
TRAIN_CLIP_SECS = 4.0
USE_AMP = True

# ==== usually unchanged ====
SR = 16000
N_FFT = 512
HOP = 256
WIN = torch.hann_window(N_FFT)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# -------------------- utils --------------------
def stft(x):
    return torch.stft(x, n_fft=N_FFT, hop_length=HOP, win_length=N_FFT,
                      window=WIN.to(x.device), return_complex=True, center=True)

def istft(S, length):
    return torch.istft(S, n_fft=N_FFT, hop_length=HOP, win_length=N_FFT,
                       window=WIN.to(S.device), length=length, center=True)

def make_features(S):
    S_r, S_i, S_m = S.real, S.imag, torch.abs(S)
    return torch.stack([S_r.permute(0,2,1), S_i.permute(0,2,1), S_m.permute(0,2,1)], dim=1)

def ensure_len(x, L):
    if len(x) >= L: return x[:L]
    out = np.zeros(L, dtype=x.dtype); out[:len(x)] = x
    return out

def pad_to_multiple(x, multiple):
    return x if x % multiple == 0 else x + (multiple - x % multiple)

def sisnr(x, s, eps=1e-8):
    s_zm, x_zm = s - s.mean(-1, True), x - x.mean(-1, True)
    proj = (torch.sum(x_zm*s_zm, -1, True) / (torch.sum(s_zm**2, -1, True)+eps)) * s_zm
    e_noise = x_zm - proj
    ratio = (torch.sum(proj**2, -1) + eps) / (torch.sum(e_noise**2, -1) + eps)
    return 10 * torch.log10(ratio + eps)

def sdr(x, s, eps=1e-8):
    num = torch.sum(s**2, -1)
    den = torch.sum((s-x)**2, -1) + eps
    return 10 * torch.log10(num/den)

# -------------------- data --------------------
class PairList:
    def __init__(self, noisy_dir, clean_dir):
        self.pairs = []
        for n in sorted(glob.glob(os.path.join(noisy_dir, "*.wav"))):
            name = os.path.basename(n)
            c = os.path.join(clean_dir, name)
            if os.path.exists(c): self.pairs.append((n, c))
    def __len__(self): return len(self.pairs)
    def __getitem__(self, i): return self.pairs[i]

class VCTKDataset(Dataset):
    def __init__(self, noisy_dir, clean_dir, secs=4.0, train=True):
        self.pl = PairList(noisy_dir, clean_dir)
        self.secs, self.train = secs, train
        self.target_len = int(SR * secs)
    def __len__(self): return len(self.pl)
    def __getitem__(self, idx):
        n_path, c_path = self.pl[idx]
        noisy, _ = sf.read(n_path, dtype="float32")
        clean, _ = sf.read(c_path, dtype="float32")
        if noisy.ndim == 2: noisy = noisy[:,0]
        if clean.ndim == 2: clean = clean[:,0]
        L = min(len(noisy), len(clean))
        noisy, clean = noisy[:L], clean[:L]
        if self.train:
            if L < self.target_len:
                noisy = ensure_len(noisy, self.target_len)
                clean = ensure_len(clean, self.target_len)
            else:
                st = random.randint(0, L - self.target_len)
                noisy, clean = noisy[st:st+self.target_len], clean[st:st+self.target_len]
        return torch.from_numpy(noisy), torch.from_numpy(clean), os.path.basename(n_path)

def collate_train(b):
    n,c,_=zip(*b); return torch.stack(n), torch.stack(c)

def collate_test(b):
    out=[]
    for n,c,name in b:
        L=max(len(n),len(c)); Lp=pad_to_multiple(L,HOP)
        n=ensure_len(n.numpy(),Lp); c=ensure_len(c.numpy(),Lp)
        out.append((torch.from_numpy(n),torch.from_numpy(c),name,L))
    return out

def train_one_epoch(model, loader, opt,  scaler=None):
    model.train()
    tot_loss = 0.0
    tot_si   = 0.0
    seen     = 0

    for noisy, clean in loader:
        noisy = noisy.to(DEVICE, non_blocking=True)
        clean = clean.to(DEVICE, non_blocking=True)

        use_amp = (scaler is not None) and noisy.is_cuda
        with torch.cuda.amp.autocast(enabled=use_amp):
            # 1) STFT
            S_noisy = stft(noisy)
            S_clean = stft(clean)

            # 2) features (B,3,T,F)
            feats = make_features(S_noisy)

            # 3) forward
            M_hat = model(feats)  # (B,2,T,F)

            # 4) complex mask and spectral estimate
            Mr = M_hat[:, 0].permute(0, 2, 1).contiguous()
            Mi = M_hat[:, 1].permute(0, 2, 1).contiguous()
            M_hat_c = torch.complex(Mr, Mi)                  # (B,F,T)
            S_hat   = M_hat_c * S_noisy                      # (B,F,T)

            # 5) spectral loss (|S|^0.3)
            mag_hat = torch.abs(S_hat).clamp_min(1e-12).pow(0.3)
            mag_ref = torch.abs(S_clean).clamp_min(1e-12).pow(0.3)
            loss_mag = torch.mean((mag_hat - mag_ref) ** 2)

            # 6) waveform reconstruction + L1 loss
            est = istft(S_hat, length=noisy.shape[-1])       # (B, L)
            loss_wav = torch.mean(torch.abs(est - clean))

            # 7) total loss
            loss = 0.5 * loss_mag + 0.5 * loss_wav

        opt.zero_grad(set_to_none=True)

        if scaler is not None and noisy.is_cuda:
            scaler.scale(loss).backward()
            scaler.unscale_(opt)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            scaler.step(opt)
            scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            opt.step()

        # metrics (no grad)
        bsz = noisy.size(0)
        with torch.no_grad():
            si = sisnr(est, clean).mean().item()
            tot_si   += si   * bsz
            tot_loss += loss.item() * bsz
            seen     += bsz

    avg_loss = tot_loss / max(1, seen)
    avg_si   = tot_si   / max(1, seen)
    return avg_loss, avg_si


@torch.no_grad()
def test_and_save(model, loader, out_dir):
    model.eval()
    Path(out_dir).mkdir(parents=True, exist_ok=True)

    pesq_metric = PerceptualEvaluationSpeechQuality(16000, mode="wb").to(DEVICE)
    stoi_metric = ShortTimeObjectiveIntelligibility(16000, extended=False).to(DEVICE)

    tot_si = tot_sd = tot_pesq = tot_stoi = 0.0
    count = 0

    for batch in loader:
        # batch_size == 1 unpack
        if isinstance(batch, list):
            noisy, clean, name, L_raw = batch[0]
        elif isinstance(batch, (tuple, list)) and len(batch) == 4:
            noisy, clean, name, L_raw = batch
        else:
            raise ValueError("Unexpected batch format for batch_size=1")

        if noisy.dim() == 1: noisy = noisy.unsqueeze(0)
        if clean.dim() == 1: clean = clean.unsqueeze(0)

        noisy = noisy.to(DEVICE)
        clean = clean.to(DEVICE)

        S_noisy = stft(noisy)                   # (1, F, T) complex
        feats   = make_features(S_noisy)        # (1, 3, T, F)
        M_hat   = model(feats)                  # (1, 2, T, F)

        Mr    = M_hat[:, 0].permute(0, 2, 1)    # (1, F, T)
        Mi    = M_hat[:, 1].permute(0, 2, 1)
        S_hat = torch.complex(Mr, Mi) * S_noisy
        est   = istft(S_hat, length=noisy.shape[-1])      # (1, L)

        Li   = int(L_raw) if not torch.is_tensor(L_raw) else int(L_raw.item())
        esti = est[0, :Li]
        clni = clean[0, :Li]

        si   = sisnr(esti.unsqueeze(0), clni.unsqueeze(0)).item()
        sd   = sdr(  esti.unsqueeze(0), clni.unsqueeze(0)).item()
        pesq = pesq_metric(esti.unsqueeze(0), clni.unsqueeze(0)).item()
        stoi = stoi_metric(esti.unsqueeze(0), clni.unsqueeze(0)).item()

        tot_si += si; tot_sd += sd
        tot_pesq += pesq; tot_stoi += stoi
        count += 1

        name_i = name if isinstance(name, str) else str(name)
        if not name_i.lower().endswith(".wav"):
            name_i += ".wav"
        sf.write(os.path.join(out_dir, name_i),
                 esti.detach().cpu().to(torch.float32).numpy(), SR)

    avg_si   = tot_si   / max(1, count)
    avg_sd   = tot_sd   / max(1, count)
    avg_pesq = tot_pesq / max(1, count)
    avg_stoi = tot_stoi / max(1, count)

    print(f"[Metrics] PESQ={avg_pesq:.3f}, STOI={avg_stoi:.3f}, "
          f"SiSNR={avg_si:.2f} dB, SDR={avg_sd:.2f} dB")

    return avg_si, avg_sd

def count_params(model, only_trainable=True):
    ps = [p.numel() for p in model.parameters() if (p.requires_grad or not only_trainable)]
    n = sum(ps)
    def pretty(x):
        if x >= 1_000_000: return f"{x/1_000_000:.3f} M"
        if x >= 1_000:     return f"{x/1_000:.3f} K"
        return str(x)
    print(f"Parameters ({'trainable' if only_trainable else 'all'}): {n}  ({pretty(n)})")
    return n

# -------------------- main --------------------
def main():
    print(f"Device={DEVICE}, AMP={USE_AMP}")
    train_noisy = os.path.join(DATA_ROOT,"train","noisy")
    train_clean = os.path.join(DATA_ROOT,"train","clean")
    test_noisy  = os.path.join(DATA_ROOT,"test","noisy")
    test_clean  = os.path.join(DATA_ROOT,"test","clean")

    ds_tr = VCTKDataset(train_noisy, train_clean, secs=TRAIN_CLIP_SECS, train=True)
    ds_te = VCTKDataset(test_noisy, test_clean, secs=TRAIN_CLIP_SECS, train=False)

    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_train)
    dl_te = DataLoader(ds_te, batch_size=1, shuffle=False, num_workers=1, collate_fn=collate_test)

    model = UNetDPT().to(DEVICE)
    count_params(model)
    opt = torch.optim.Adam(model.parameters(), lr=LR)
    sch = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)
    scaler = torch.cuda.amp.GradScaler(enabled=(USE_AMP and DEVICE=="cuda"))

    print(f"Train pairs={len(ds_tr)}, Test pairs={len(ds_te)}")
    for ep in range(1,EPOCHS+1):
        loss, sis = train_one_epoch(model, dl_tr, opt, scaler)
        sch.step()
        print(f"[Epoch {ep:02d}] loss={loss:.4f}, SiSNR={sis:.2f} dB, lr={opt.param_groups[0]['lr']:.2e}")

    avg_si, avg_sd = test_and_save(model, dl_te, OUT_DIR)
    os.makedirs(os.path.dirname(CKPT_PATH), exist_ok=True)
    torch.save({"model": model.state_dict()}, CKPT_PATH)
    print(f"Checkpoint saved to: {CKPT_PATH}")
    print(f"Enhanced audio saved to: {OUT_DIR}")

if __name__ == "__main__":
    main()
